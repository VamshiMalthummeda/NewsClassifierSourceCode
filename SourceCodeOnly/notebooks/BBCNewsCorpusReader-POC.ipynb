{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pickle\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import tabulate\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from yellowbrick.text.freqdist import FreqDistVisualizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = r'D:\\NLP\\BBC'\n",
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "zipFile = url[url.rfind(\"/\",0) + 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownloadandExtract(dir_path,file_name,url):\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        zfile = open(file_path, 'wb')\n",
    "        zfile.write(resp.content)\n",
    "        zfile.close()\n",
    "        with zipfile.ZipFile(file_path,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(dir_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DownloadandExtract(dirpath,zipFile,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBCNewsCorpusReader(CategorizedPlaintextCorpusReader):\n",
    "    \n",
    "    def resolve(self, fileids, categories):\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "    \n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of the document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "    \n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tuples, the fileid and size on disk of the file.\n",
    "        This function is used to detect oddly large files in the corpus.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, getting every path and computing filesize\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)\n",
    "    \n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.time()\n",
    "\n",
    "        # Structures to perform counting.\n",
    "        counts  = nltk.FreqDist()\n",
    "        tokens  = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "\n",
    "            for sent in para:\n",
    "                counts['sents'] += 1\n",
    "\n",
    "                for word in sent:\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "\n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "        n_topics  = len(self.categories(self.resolve(fileids, categories)))\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files':  n_fileids,\n",
    "            'topics': n_topics,\n",
    "            'paras':  counts['paras'],\n",
    "            'sents':  counts['sents'],\n",
    "            'words':  counts['words'],\n",
    "            'vocab':  len(tokens),\n",
    "            'lexdiv': float(counts['words']) / float(len(tokens)),\n",
    "            'ppdoc':  float(counts['paras']) / float(n_fileids),\n",
    "            'sppar':  float(counts['sents']) / float(counts['paras']),\n",
    "            'secs':   time.time() - started,\n",
    "        }            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[\\w_\\s]+/[\\w\\s\\d\\-]+\\.txt'\n",
    "CAT_PATTERN = r'([\\w_\\s]+)/.*'\n",
    "corpus = BBCNewsCorpusReader(r'D:\\NLP\\BBC\\bbc',DOC_PATTERN, cat_pattern=CAT_PATTERN,encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'files': 2225,\n",
       " 'topics': 5,\n",
       " 'paras': 12772,\n",
       " 'sents': 43990,\n",
       " 'words': 1007937,\n",
       " 'vocab': 33754,\n",
       " 'lexdiv': 29.861260887598508,\n",
       " 'ppdoc': 5.740224719101124,\n",
       " 'sppar': 3.4442530535546507,\n",
       " 'secs': 4.6499903202056885}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    \n",
    "    def __init__(self, corpus, target=None,**kwargs):\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "        \n",
    "    def fileids(self, fileids=None, categories=None):\n",
    "        fileids = self.corpus.resolve(fileids, categories)\n",
    "        if fileids:\n",
    "            return fileids\n",
    "        return self.corpus.fileids()\n",
    "    \n",
    "    def abspath(self, fileid):\n",
    "        # Find the directory, relative to the corpus root.\n",
    "        parent = os.path.relpath(\n",
    "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
    "        )\n",
    "\n",
    "        # Compute the name parts to reconstruct\n",
    "        basename  = os.path.basename(fileid)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "\n",
    "        # Create the pickle file extension\n",
    "        basename  = name + '.pickle'\n",
    "\n",
    "        # Return the path to the file relative to the target.\n",
    "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
    "    \n",
    "    def process(self, fileid):\n",
    "        \"\"\"For a single file, checks the location on disk to ensure no errors,\n",
    "        uses +tokenize()+ to perform the preprocessing, and writes transformed\n",
    "        document as a pickle to target location.\n",
    "        \"\"\"\n",
    "        # Compute the outpath to write the file to.\n",
    "        target = self.abspath(fileid)\n",
    "        parent = os.path.dirname(target)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "\n",
    "        # Make sure that the parent is a directory and not a file\n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError(\n",
    "                \"Please supply a directory to write preprocessed data to.\"\n",
    "            )\n",
    "\n",
    "        # Create a data structure for the pickle\n",
    "        document = list(self.tokenize(fileid))\n",
    "        # Open and serialize the pickle to disk\n",
    "        with open(target, 'wb') as f:\n",
    "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Clean up the document\n",
    "        del document\n",
    "    \n",
    "    def transform(self, fileids=None, categories=None):\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        # Resolve the fileids to start processing\n",
    "        for fileid in self.fileids(fileids, categories):\n",
    "            yield self.process(fileid)\n",
    "            \n",
    "    def tokenize(self, fileid):\n",
    "        for paragraph in self.corpus.paras(fileids=fileid):\n",
    "            yield [\n",
    "                pos_tag(sent) \n",
    "                for sent in paragraph\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, ..., None, None, None], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = r'D:\\NLP\\BBC\\processed'\n",
    "processed_corpus = Preprocessor(corpus,target)\n",
    "vect = np.vectorize(processed_corpus.process)\n",
    "vect(processed_corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickledCorpusReader(BBCNewsCorpusReader):\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        # Load one pickled document into memory at a time.\n",
    "        for path in self.abspaths(fileids):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "            \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for para in doc:\n",
    "                yield para\n",
    "    \n",
    "    def tagged(self, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for tagged_token in sent:\n",
    "                yield tagged_token\n",
    "                \n",
    "    def words(self, fileids=None, categories=None):\n",
    "        for tagged in self.tagged(fileids, categories):\n",
    "            yield tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[\\w_\\s]+/[\\w\\s\\d\\-]+\\.pickle'\n",
    "CAT_PATTERN = r'([\\w_\\s]+)/.*'\n",
    "target = r'D:\\NLP\\BBC\\processed'\n",
    "processed_corpus = PickledCorpusReader(target,DOC_PATTERN, cat_pattern=CAT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "    \n",
    "    def normalize(self, allDocuments):\n",
    "        docsAsList = list()\n",
    "        for catDocs in allDocuments:\n",
    "            for document in catDocs:\n",
    "                wordsList = list()\n",
    "                for paragraph in document:\n",
    "                    for sentence in paragraph:\n",
    "                        for token, tag in sentence:\n",
    "                            if not self.is_punct(token) and not self.is_stopword(token) and not token.isdigit() and len(token) > 1:\n",
    "                                wordsList.append(self.lemmatize(token, tag).lower())\n",
    "                docsAsList.append(\" \".join(wordsList))\n",
    "        return docsAsList\n",
    "    \n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusLoader(object):\n",
    "    def __init__(self, reader, folds=12, shuffle=True, categories=None):\n",
    "        self.reader = reader\n",
    "        self.folds  = KFold(n_splits=folds, shuffle=shuffle)\n",
    "        self.all_files  = np.asarray(self.reader.fileids(categories=categories))\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.fileids(), self.labels(), test_size=0.33,shuffle=shuffle)\n",
    "        self.cv_files = np.asarray(self.X_train)\n",
    "        self.test_files = np.asarray(self.X_test)\n",
    "        \n",
    "    def fileids(self,file_type=\"All\",idx=None):\n",
    "        files = []\n",
    "        if file_type == \"TEST\":\n",
    "            files = self.test_files \n",
    "        elif file_type == \"CV\":\n",
    "            files = self.cv_files\n",
    "        else:\n",
    "            files = self.all_files\n",
    "        if idx is None:\n",
    "            return files\n",
    "        return files[idx]\n",
    "    \n",
    "    def documents(self,file_type=\"All\",idx=None):\n",
    "        for fileid in self.fileids(file_type,idx):\n",
    "            yield list(self.reader.docs(fileids=[fileid]))\n",
    "            \n",
    "    def labels(self,file_type=\"All\",idx=None):\n",
    "        return [\n",
    "            self.reader.categories(fileids=[fileid])[0]\n",
    "            for fileid in self.fileids(file_type,idx)\n",
    "        ]\n",
    "    def __iter__(self):\n",
    "        file_type = \"CV\"\n",
    "        for train_index, test_index in self.folds.split(self.X_train):\n",
    "            X_train_fold = self.documents(file_type,train_index)\n",
    "            y_train_fold = self.labels(file_type,train_index)\n",
    "\n",
    "            X_test_fold = self.documents(file_type,test_index)\n",
    "            y_test_fold = self.labels(file_type,test_index)\n",
    "\n",
    "            yield X_train_fold, X_test_fold, y_train_fold, y_test_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=0.1,smooth_idf=True,norm='l2',sublinear_tf=True)\n",
    "def create_pipeline(estimator):\n",
    "    steps = [\n",
    "        ('normalize', TextNormalizer()),\n",
    "        ('vectorize', vectorizer),\n",
    "        ('classifier',estimator)\n",
    "    ]\n",
    "    return Pipeline(steps)\n",
    "\n",
    "models = []\n",
    "for form in (MultinomialNB,):\n",
    "    models.append(create_pipeline(form()))\n",
    "loader = CorpusLoader(processed_corpus)\n",
    "scores = []\n",
    "for model in models:\n",
    "    search = GridSearchCV(model, cv=12,param_grid={\n",
    "        'vectorize__max_df' : [1.0,0.9,0.8,0.7,0.6,0.5],\n",
    "        'vectorize__min_df' : [0.4,0.3,0.2,0.15,0.1,0.05],\n",
    "        'vectorize__smooth_idf' : [True,False],\n",
    "        'vectorize__norm' : ['l1','l2'],\n",
    "        'vectorize__sublinear_tf' : [True,False]\n",
    "        })\n",
    "    search.fit([list([doc]) for doc in loader.reader.docs(loader.X_train)],y_train)\n",
    "    y_pred = search.predict([list([doc]) for doc in loader.reader.docs(loader.X_test)])\n",
    "    print(classification_report(y_test, y_pred, labels=corpus.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
